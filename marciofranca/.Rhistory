votos_2018 <- electionsBR::details_mun_zone_fed(2018)
library(tidyverse)
glimpse(votos_2018)
detalhes_2018 <- votos_2018
View(detalhes_2018)
pesquisas_2018 <- electionsBR::party_mun_zone_fed(2018)
glimpse(pesquisas_2018)
afiliacao_eleitores <- electionsBR::voter_affiliation(party = "PT")
afiliacao_eleitores <- electionsBR::voter_affiliation(party = "PT", uf="BA")
glimpse(afiliacao_eleitores)
View(afiliacao_eleitores)
save(afiliacao_eleitores, perfil_eleitores, pesquisas_2018, file="dados_eleicoes.RDat")
save(detalhes_2018, afiliacao_eleitores, perfil_eleitores, pesquisas_2018, file="dados_eleicoes.RDat")
save(detalhes_2018, afiliacao_eleitores, pesquisas_2018, file="dados_eleicoes.RDat")
library(tabulizer)
library(dplyr)
library(readr)
library(tidyverse)
setwd("/Users/isiscosta/RScript/Scite")
filenames <- c("FOS_003_18_16.pdf","FOS_007_878ed827-e2ff-002a-b31b-7f50eb50bb12.pdf","FOS_008_22735.pdf","FOS_009_1342-1349.pdf")
filename <- filenames[1]
?extract_metadata
extract_metadata(filename)
?extract_tables
df <- extract_tables(filename, output = "data.frame")
head(df)
View(df)
class(df)
head(df[1])
head(df[2])
head(df[[1]])
head(df[[2]])
filename
texts <- extract_text(filename)
class(texts)
texts
length(texts)
class(texts)
texts[1]
texts[[1]]
texts
stringr::str_detect(texts, pattern = "Table")
stringr::str_detect_all(texts, pattern = "Table")
stringr::str_extract_all(texts, pattern = "Table")
tabulizer::extract_tables(filename, output = "csv",outdir = getwd())
filenames <- c("FOS_003_18_16.pdf","FOS_007_878ed827-e2ff-002a-b31b-7f50eb50bb12.pdf","FOS_008_22735.pdf","FOS_009_1342-1349.pdf")
for(filename in filenames){
tabulizer::extract_tables(filename, output = "csv",outdir = getwd())
}
for(filename in filenames){
tabulizer::extract_tables(filename, output = "csv",outdir = getwd())
}
filenames <- c("FOS_003_18_16.pdf","FOS_007_87812.pdf","FOS_008_22735.pdf","FOS_009_1342-1349.pdf")
for(filename in filenames){
tabulizer::extract_tables(filename, output = "csv",outdir = getwd())
}
library(tidyverse)
library(lubridate)
dat <- readRDS("data/tweets_MF.rds")
setwd("/Users/isiscosta/RScript/marciofranca/")
dat <- readRDS("data/tweets_MF.rds")
install.packages("gTrendsR")
install.packages("gtrendsR")
library(gtrendsR)
?gtrends
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 12-m",
hl = "pt-br",
low_search_volume = TRUE,
tz = -120)
language_codes$code
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 12-m",
hl = "pt",
low_search_volume = TRUE,
tz = -120)
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 12-m",
hl = "pt",
low_search_volume = TRUE)
class(trendsMF)
names(trendsMF)
trendsMF$interest_by_city
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 12-m",
hl = "pt")
trendsMF
data("categories")
categories
categories[grepl("^Politics")]
categories[grepl("^Politics"),]
categories[grepl("^Politics",categories$name),]
categories[grepl("^Sport", categories$name), ]
categories[grepl("^Politics",categories$name),]
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 12-m",
hl = "pt",
category = 396)
trendsMF
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "all",
hl = "pt",
category = 396)
trendsMF
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 1-m",
hl = "pt",
category = 396)
trendsMF
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 3-m",
hl = "pt",
category = 396)
trendsMF
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "today 12-m",
hl = "pt",
category = 396)
trendsMF
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "2019-01-01 2020-03-01",
hl = "pt",
category = 396)
trendsMF
trendsMF <- gtrends("Márcio França",
geo = "BR",
time = "2019-01-01 2020-03-01",
category = 396,
onlyInterest = FALSE)
trendsMF
setwd("/Users/isiscosta/RScript/marciofranca/")
dat <- readRDS("data/tweets_MF.rds")
dat <- dat %>% mutate(data = round_date(ymd_hms(created_at),"day"))
View(dat)
datSP <- readRDS("data/tweets_SP.rds")
datSP <- datSP %>%
mutate(data = round_date(ymd_hms(created_at),"day"))
View(datSP)
datSP %>% count(retweet_location)
datSP %>% count(retweet_location) %>% arrange(n) %>% tail(20)
datSP %>% count(location) %>% arrange(n) %>% tail(20)
datSP <- datSP %>%
filter(
location %in% c("São Paulo, Brasil","São Paulo", "Sao Paulo, Brazil", "Taubaté, São Paulo"
)
)
datSP <- datSP %>%
mutate(data = round_date(ymd_hms(created_at),"day"))
View(datSP)
datSP %>% count(user_id)
datSP %>% count(user_id) %>% arrange(n)
datSP %>% count(user_id) %>% arrange(n) %>% tail(10)
datSP %>% count(user_id) %>% arrange(n) %>% tail(20)
datSP %>% count(screen_name) %>% arrange(n) %>% tail(20)
#########
library(dplyr)
library(tidytext)
install.packages("tidytext")
library(tidytext)
View(datSP)
## networks
datSP %>% select(text) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
## networks
datSP %>% select(text) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 4)
## networks
stopwords::stopwords(language = "pt")
## networks
badwords <- stopwords::stopwords(language = "pt")
datSP %>% filter(!text %in% badwords) %>%
select(text) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 4)
datSP %>% filter(!text %in% badwords)
datSP %>% filter(!text %in% badwords)%>%
select(text) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 4)
datSP %>% filter(!text %in% badwords)%>%
unnest_tokens(bigram, text, token = "ngrams", n = 4)
datSP %>% filter(!text %in% badwords)
datSP %>% filter(!text %in% badwords) %>%
select(text)
datSP %>% unnest_tokens(bigram, text, token = "ngrams", n = 1)
datSP <- readRDS("data/tweets_SP.rds")
datSP <- datSP %>%
filter(
location %in% c("São Paulo, Brasil","São Paulo", "Sao Paulo, Brazil", "Taubaté, São Paulo"
)
)
datSP <- datSP %>%
mutate(data = round_date(ymd_hms(created_at),"day"))
## networks
badwords <- stopwords::stopwords(language = "pt")
datSP %>% unnest_tokens(bigram, text, token = "ngrams", n = 1)
datSP %>% unnest_tokens(bigram, datSP$text, token = "ngrams", n = 1)
datSP
datSP <- readRDS("data/tweets_SP.rds")
datSP <- datSP %>%
filter(
location %in% c("São Paulo, Brasil","São Paulo", "Sao Paulo, Brazil", "Taubaté, São Paulo"
)
)
datSP <- datSP %>%
mutate(data = round_date(ymd_hms(created_at),"day"))
datSP
datSP
datSP %>% unnest_tokens(bigram, text, token = "ngrams", n = 1) %>%
anti_join(badwords)
datSP %>% filter(!is.na(text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 1) %>%
anti_join(badwords)
datSP %>% filter(!is.na(text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 1)
datSP <- readRDS("data/tweets_SP.rds")
datSP <- datSP %>%
filter(
location %in% c("São Paulo, Brasil","São Paulo", "Sao Paulo, Brazil", "Taubaté, São Paulo"
)
)
datSP <- datSP %>%
mutate(data = round_date(ymd_hms(created_at),"day"))
## networks
badwords <-  stopwords::stopwords(language = "pt")
datSP %>% filter(!is.na(text)) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 1)
datSP %>%   unnest_tokens(bigram, text, token = "ngrams", n = 1)
View(datSP)
datSP %>%   unnest_tokens(bigram, datSP$text, token = "ngrams", n = 1)
datSP %>%   unnest_tokens(bigram, datSP$text, token = "ngrams", n = 2)
datSP$text
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 1)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
%>% count(bigram, sort = TRUE)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)%>%
separate(bigram, c("word1", "word2"), sep = " ")
## networks
badwords <-  stopwords::stopwords(language = "pt")
badwords <- c(badwords, "https","t.co")
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)%>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% badwords) %>%
filter(!word2 %in% badwords)%>%
count(word1, word2, sort = TRUE)%>%
unite(bigram, word1, word2, sep = " ")
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)%>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% badwords) %>%
filter(!word2 %in% badwords)%>%
count(word1, word2, sort = TRUE)%>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(n) %>% tail(20)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)%>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% badwords) %>%
filter(!word2 %in% badwords)%>%
count(word1, word2, sort = TRUE)%>%
unite(bigram, word1, word2, sep = " ")
library(igraph)
install.packages("igraph")
library(igraph)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)%>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% badwords) %>%
filter(!word2 %in% badwords)%>%
count(word1, word2, sort = TRUE)%>%
unite(bigram, word1, word2, sep = " ")
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 20) %>%
graph_from_data_frame()
install.packages("ggraph")
library(ggraph)
set.seed(2017)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 20) %>%
graph_from_data_frame()%>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name),
vjust = 1,
hjust = 1))
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 20) %>%
graph_from_data_frame()%>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name),
vjust = 1,
hjust = 1)
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 20) %>%
graph_from_data_frame()%>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 20) %>%
graph_from_data_frame()%>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
datSP %>% select(text) %>%  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 20) %>%
graph_from_data_frame()%>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = 0.5), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
library(stringr)
datSP %>%
count_bigrams()
count_bigrams <- function(dataset) {
dataset %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
}
datSP %>%
count_bigrams()
datSP %>% select(text) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 20) %>%
graph_from_data_frame()%>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = 0.5), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
library(rtweet)
library(tidyverse)
library(lubridate)
setwd("/Users/isiscosta/RScript/marciofranca/")
setwd("/Users/isiscosta/RScript/objectiva2020/marciofranca")
terms1 <- c("Prefeitura","Prefeito")
terms2 <- c("PSB","PSDB","PT","PSOL","MDB","PSD","PDT")
terms3 <- "São Paulo"
terms4 <- c("Márcio França","João Dória","Mara Gabrili","Bruno Covas","Marta Suplicy","Datena","Boulos","Jair Bolsonaro")
usuarios <- c("marciofrancabr","jdoriajr","brunocovas","DatenaOficial","LulaOficial","martasuplicy_","GuilhermeBoulos","maragabrilli","jairbolsonaro")
for(term2 in terms2){
tweets <- tibble()
for(term1 in terms1){
cat(paste(term1,term2),sep="\n")
tmp_tweets <- search_tweets(paste(term1,term2,terms3),n=1000,retryonratelimit = TRUE)
tmp_tweets$term1 <- rep(term1,nrow(tmp_tweets))
tmp_tweets$term2 <- rep(term2,nrow(tmp_tweets))
tweets <- rbind(tweets,tmp_tweets)
}
saveRDS(tweets, file=paste0("data/",paste(term2,sep="_"),".rds"))
}
for(term2 in terms4){
tweets <- tibble()
for(term1 in terms1){
cat(paste(term1,term2),sep="\n")
tmp_tweets <- search_tweets(paste(term1,term2,terms3),n=1000,retryonratelimit = TRUE)
tmp_tweets$term1 <- rep(term1,nrow(tmp_tweets))
tmp_tweets$term2 <- rep(term2,nrow(tmp_tweets))
tweets <- rbind(tweets,tmp_tweets)
}
saveRDS(tweets, file=paste0("data/",paste(term2,sep="_"),".rds"))
}
usuarios <- c("marciofrancabr","jdoriajr","brunocovas","DatenaOficial","LulaOficial","martasuplicy_","GuilhermeBoulos","maragabrilli","jairbolsonaro")
tweets <- tibble()
for(myuser in usuarios){
cat(paste(user),sep="\n")
tmp_tweets <- get_timeline(user = myuser, n = 300)
tweets <- rbind(tweets,tmp_tweets)
}
tweets <- tibble()
for(myuser in usuarios){
cat(paste(myuser),sep="\n")
tmp_tweets <- get_timeline(user = myuser, n = 300)
tweets <- rbind(tweets,tmp_tweets)
}
saveRDS(tweets, file=paste0("data/timeline_",paste(myuser,sep="_"),".rds"))
##### redes
followers <- tibble()
for(myuser in usuarios){
cat(myuser,sep = "\n")
tmp_followers <- get_followers(myuser, n=5000,retryonratelimit = TRUE)
tmp_followers$user <- myuser
followers <- rbind(followers,tmp_followers)
}
saveRDS(followers, file=paste0("data/followers.rds"))
dat <- readRDS("timeline_politicos.rds")
dat <- readRDS("data/timeline_politicos.rds")
head(dat)
names(dat)
library(tidyverse)
library(lubridate)
dat  %>%
mutate(date = round_date(ymd_hms(created_at),"day")) %>%
filter(date > ymd("2019-06-01")) %>%
group_by(date,screen_name) %>%
summarise(total = mean(favorites_count)) %>%
ggplot(aes(x = date,y=total,col=screen_name)) +
geom_line(stat="identity") +
facet_wrap(~screen_name,scales="free_y")
dat  %>%
mutate(date = round_date(ymd_hms(created_at),"day")) %>%
filter(date > ymd("2019-06-01")) %>%
group_by(date,screen_name) %>%
summarise(total = mean(favorite_count)) %>%
ggplot(aes(x = date,y=total,col=screen_name)) +
geom_line(stat="identity") +
facet_wrap(~screen_name,scales="free_y")
dat  %>%
mutate(date = round_date(ymd_hms(created_at),"day")) %>%
filter(date > ymd("2019-10-01")) %>%
group_by(date,screen_name) %>%
summarise(total = mean(favorite_count)) %>%
ggplot(aes(x = date,y=total,col=screen_name)) +
geom_line(stat="identity") +
facet_wrap(~screen_name,scales="free_y")
?get_mentions
##### citations
usuarios <- c("@marciofrancabr","@jdoriajr","@brunocovas","@DatenaOficial","@martasuplicy_","@GuilhermeBoulos","@maragabrilli")
tweets <- tibble()
getwd()
for(myuser in usuarios){
cat(paste(myuser),sep="\n")
tmp_tweets <- get_timeline(user = myuser, n = 300)
tweets <- rbind(tweets,tmp_tweets)
}
saveRDS(tweets, file=paste0("data/mentions_politicos.rds"))
dat <- readRDS("data/mentions_politicos.rds")
head(dat)
dat %>% count(screen_name)
##### usuarios
usuarios <- c("marciofrancasp","jdoriajr","brunocovas","DatenaOficial","martasuplicy_","GuilhermeBoulos","maragabrilli")
tweets <- tibble()
for(myuser in usuarios){
cat(paste(myuser),sep="\n")
tmp_tweets <- get_timeline(user = myuser, n = 300)
tweets <- rbind(tweets,tmp_tweets)
}
saveRDS(tweets, file=paste0("data/timeline_politicos.rds"))
##### redes
followers <- tibble()
for(myuser in usuarios){
cat(myuser,sep = "\n")
tmp_followers <- get_followers(myuser, n=5000,retryonratelimit = TRUE)
tmp_followers$user <- myuser
followers <- rbind(followers,tmp_followers)
}
saveRDS(followers, file=paste0("data/followers.rds"))
##### citations
usuarios <- c("@marciofrancasp","@jdoriajr","@brunocovas","@DatenaOficial","@martasuplicy_","@GuilhermeBoulos","@maragabrilli")
tweets <- tibble()
for(myuser in usuarios){
cat(paste(myuser),sep="\n")
tmp_tweets <- get_timeline(user = myuser, n = 300)
tweets <- rbind(tweets,tmp_tweets)
}
saveRDS(tweets, file=paste0("data/mentions_politicos.rds"))
dat <- readRDS("data/timeline_politicos.rds")
library(tidyverse)
library(lubridate)
dat  %>%
mutate(date = round_date(ymd_hms(created_at),"day")) %>%
filter(date > ymd("2019-10-01")) %>%
group_by(date,screen_name) %>%
summarise(total = mean(favorite_count)) %>%
ggplot(aes(x = date,y=total,col=screen_name)) +
geom_line(stat="identity") +
facet_wrap(~screen_name,scales="free_y")
dat <- readRDS("data/mentions_politicos.rds")
dat %>% count(screen_name)
dat <- readRDS("data/mentions_politicos.rds")
dat %>% count(screen_name)
dat %>% group_by(screen_name) %>% summarise(total = sum(favorite_count))
dat %>% group_by(screen_name) %>% summarise(total = sum(retweet_count))
